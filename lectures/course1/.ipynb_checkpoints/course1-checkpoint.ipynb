{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left off first series of lectures\n",
    "<h6>What is a NN?</h6>\n",
    "<img src=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Supervised Learning w NN</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Why is DL taking off?</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Logistic Regression as NN. Binary Classification?</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Logistic Regression</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Gradient Descent</h6>\n",
    "Do this and verify there is a $dZ=A^{[2]}-Y$ term here. It comes from\n",
    "taking the derivative of $\\frac{1}{2}(A^{[2]}-Y)^{[2]}$ dZ/dA. and then \n",
    "using chain rule from A. A is the output of the network which is compared\n",
    "to the training labels to determine error or not. Minimize the MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Derivatives</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>More derivative examples</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Computation Graph?</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Derivatives w/computation Graph?</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Gradient Desctne on m examples</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Python and Vectorizatioin Vectorization</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>more vectorization examples</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>vectorizing logistic regression</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>vectorizing logistic regressions gradient output</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Broadcasting in python</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>A note on pythnon numpy vectors?</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Explanation of logistic regression cost function</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Shallow Network-NN Overview</h6>\n",
    "<img src=\"week3a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Shallow Network-NN Representation</h6>\n",
    "<img src=\"week3b.png\">\n",
    "<img src=\"week3c.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Shallow Network-computing output</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Shallow Network-vectorizing</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Shallow Network-explanation for vectorized</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C1M1.pdf\t classification_kiank.png   README.md\t       week3questions\r\n",
      " C1M2.pdf\t course1.ipynb\t\t    w3activation.png   week4a.png\r\n",
      "'C1M3 (1).pdf'\t course1_week1and2.ipynb    week3a.png\t       week4b.png\r\n",
      " C1M3.pdf\t planar.ipynb\t\t    week3b.png\t       week4c.png\r\n",
      " C1M4.pdf\t practice_deep.ipynb\t    week3c.png\t       week4d.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Shallow Network-activation functions</h6>\n",
    "<img src=\"w3activation.png\">\n",
    "What are the tradeoffs? \n",
    "<li>never use sigmoid except for binary classification output or logistic regression. tanh is superior\n",
    "to sigmoid, output beween plus/minus 1</li>\n",
    "<li>some say the leaky relu works better with a small value and you can make that a learnable parameter</li>\n",
    "<li>Use leaky relu to reduce vanishing gradient. Show this mathematically. Sparsity and reduced likelihood\n",
    "of vanishing gradient. Sparsity means some of the weights are 0 or close to 0 which is like dropout and\n",
    "would reduce overfitting</li>\n",
    "<h6>what is the advangate of a negative slope in leaky relu? </h6>\n",
    "<h6>Is there a difference in activation functions loss for regression vs. classification?</h6>\n",
    "<p>yes one is MSE for regression, another is binary CE loss for 2 class or softmax for multinomial</p>\n",
    "<h6>softmax is an activation and we use binary CE as a loss function</h6>\n",
    "<h6>what is loss function for tanh activation? </h6>\n",
    "<li>The advantage of using non-saturated activation function lies in two aspects: The first is to solve the so called “exploding/vanishing gradient”. \n",
    "    The second is to accelerate the convergence speed.</li>\n",
    "    <li>From:https://arxiv.org/abs/1505.00853</li>\n",
    "\n",
    "<img src=\"w3activation2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Shallow Network-why nonlinear activation functions</h6>\n",
    "You need nonlinear to form deep network layers or else you could combine all the deep layers into 1 layer by the definition \n",
    "of linearity. The deeper network layers are needed to represent different levels of abstraction. For example in a\n",
    "DNN for a cat the earlier layers would encode edges and the later layers cats themselves. \n",
    "The standard forward propagation equations for a single layer NN. \n",
    "<img src=\"nonlinact1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Shallow Network-Derivatives of activation functions</h6>\n",
    "derivative of sigmoid is y(1-y). g'(0)=1/4. \n",
    "<img src=\"derivactiv1.png\">\n",
    "$1-tanh*2(z)$ g(0) =1, tanh(0)=0, derivative of tanh is 1-0=1 \n",
    "<img src=\"derivactiv2.png\">\n",
    "relu derivative is 1 if z>0 else 0, undefined at 0 but most sw just makes it 0. Leaky relu is 0.01 if z<0 else 1 if z>=0.\n",
    "\n",
    "<img src=\"derivactiv3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Shallow Network-Grad Descent</h6>\n",
    "Backprop computes the parameters as a function of minimizing the loss. The parameters are $W^{[1]}$,$b^{[1]}$,\n",
    "$W^{[2]}$,$b^{[2]}$. Gradient descent calculated $dW^{[1]}=\\frac{\\partial J}{\\partial W^{[1]}}$ \n",
    "and $db^{[1]}=\\frac{\\partial J}{\\partial b^{[1]}}$ Gradient descent changes the value of the W and b parameters by \n",
    "making small changes determined by the direction of the gradient and learning rate. $W^{[1]}=W^{[1]}-\\alpha dW^{[1]}$\n",
    "and for the bias... $b^{[1]}=b^{[1]}-\\alpha db^{[1]}$ This is for one iteration. How to compute these partial derivatives?\n",
    "<img src=\"graddes1.png\">\n",
    "Given the linear Shallow network forward propagation equations: \n",
    "    <p>$Z^{[1]}=W^{[1]}X+b^{[1]}$</p>\n",
    "    <p>$A^{[1]}=g^{[1]}(Z^{[1]})$</p>\n",
    "    <p>$Z^{[2]}=W^{[2]}X+b^{[2]}$</p>\n",
    "    <p>$A^{[2]}=g^{[2]}(Z^{[2]})=\\sigma(Z^{[2]})$</p>\n",
    " The gradient descent equations are derived from the above, vectorized across teh training set:\n",
    "    Very similar to grad descent over logistic regressoin\n",
    "    <p>$dZ^{[2]}=A^{[2]}-Y$</p>\n",
    "    <p>$dW^{[2]}=\\frac{1}{m}dZ^{[2]}A^{[1]}T$</p>\n",
    "    <p>$db^{[2]}=\\frac{1}{m}np.sum(dZ^{[2]},axis=1,keep_dims=True)$</p>\n",
    "    <p>$dZ^{[1]}=W^{[2]T}dZ^{2}*g^{[1]'}(Z^{[1]})$</p> where g1 prime is the derivative of the activation function\n",
    "                    used in the hidden layer. The Z2=A2-Y asusmes sigmoid for binary classification. \n",
    "    <p>$dW^{[1]}=\\frac{1}{m}dZ^{[1]T}$</p>\n",
    "    <p>$db^{[1]}=\\frac{1}{m}np.sum(dZ^{[1]},axis=1,keep_dims=True)$</p>\n",
    "<img src=\"graddes1a.png\">                \n",
    "<img src=\"graddes2.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Shallow Network-BackProp</h6>\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Shallow Network-Random Initialization</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Xavier</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Deep Network Deep L layer BN</h6>\n",
    "<img src=\"week4a.png\">\n",
    "$a^{[l]} = g^{[l]}Z^{[l]}$\n",
    "<p></p>\n",
    "$Z^{[l]}=x^{[l]}W^{[l]}+b^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Deep Network Forward Propagation in Deep NN</h6>\n",
    "<img src=\"week4a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Deep Network Getting matrix dims right</h6>\n",
    "<img src=\"week4c.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Deep Network Why deep representations</h6>\n",
    "<img src=\"week4d.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep Network buiolding blocks of deep NN</h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep Network forward and backward prop</h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep Network parameters vs. hyperparameters</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Practice chain rule and backprop</h6>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
