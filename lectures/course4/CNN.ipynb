{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN Computer Vision</h6>\n",
    "<img src=\"cnn_cv1.png\">\n",
    "Number of parameters in CNN:\n",
    "<li>calculating convolution horizontal stride, input_size-(filter_size-1) or input_size-filter_size+1; if \n",
    "inputimage=5x5 and kernel=4x4 move 1, there are 2 convolutions. 5-4+1=2. This checks for horizontal direction. \n",
    "For vertical direction we have 2 also. m*n=2*2=4. </li>\n",
    "<li>If we want the output to be the same dimensions we can zero pad. How much do you have to zero pad\n",
    "to make the output the same dimensions as the input? For the 5x5 case we need 5 horizontal convolutions. With \n",
    "a 4x4 kernel we have 2, pad 3. 54321000. The output size after padded convolution is output_size=input_size+2*pad-1. As a test\n",
    "check our test case, input_size=5, output_size=input_size+2*pad-1; 5=5+2*(pad size)-1;Something wrong... <li>\n",
    "<li></li>\n",
    "<li>Is model size proportional to inverse of learnable parameters? </li>\n",
    "\n",
    "<h6>Output size Convolutional Layer, coding assignment uses n_H, n_W, n_C</h6>\n",
    "A volume of size W1,H1,D1 given K= num filters, F=spatial extent, S =stride, P = zero padding. \n",
    "$n_W = \\frac{W1-F+2P}{S} + 1$ and $n_H=\\frac{H1-F+2P}{S}+1$ and $D2=K$\n",
    "<h6>Parameter sharing</h6>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN Edge Detection</h6>\n",
    "Vertical edge detection bc you see the gradient go from 1 to 0 to -1. \n",
    "<img src=\"cnn_edge1.png\">\n",
    "Simpler example of vertical line convolved w/3x3 filter. Filter is lighter on left, darker on right. You see\n",
    "the convolution plotted. Lighter in middle, darker on sides. \n",
    "<img src=\"cnn_edge2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN more Edge</h6>\n",
    "If you have edges on both sides, left and right the bottom is negative 30 vs 30. \n",
    "<img src=\"cnn_edge3.png\">\n",
    "3x3 filter for vertical and horizontal edge. \n",
    "<img src=\"cnn_edge4.png\">\n",
    "Sobel filter 3x3 puts more weight to central pixel adn maybe more robust. Scharr filter has advantage in giving heavierweights \n",
    "to edges, increases the the difference. Maybe you can learn a filter instead of hand coding an image filter. These 9 number\n",
    "are parameters, tehhe backprop can learn some kernel whcih can be better than these hand coded filters. BackProp can learn different\n",
    "edges. \n",
    "<img src=\"cnn_edge5.png\">\n",
    "<h6>45 degree filter</h6>\n",
    "<img src=\"sobel45.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN padding</h6>\n",
    "2 problems with convolution, 1) each convolution shrinks imaage, teh convolution size is (n-f+1)x(n-f+1) and 2)\n",
    "the edges are underrepresente because they get less convolution, only when touched by edge. Ideally you need to move\n",
    "across teh entire filter. Size after padding is n+2p-f+1. Teh green edge pixel now is a center pixel and the edges\n",
    "are not counted less anymore. Can pad 2. He only showed pad =1. \n",
    "<img src=\"cnn_pad1.png\">\n",
    "There are 2 choices amount to pad. Valid convolution means no padding. Valid is nxn convolve w/ fxf gives n-f+1. For 6x6\n",
    "image convolved w/3x3 we get 4x4 output. Same: input size is same as output size. Want : n+2p-f+1=n;p=(f-1)/2. When f is odd\n",
    "        you can choose output size to be same as input size. For previous example he picked p=1 for filter =3x3, f... if 5x5 need\n",
    "        pad=2. By convention in computer vision f is usually odd. if F even then you need some assymetric padding. Odd filter such\n",
    "        as 3x3 you have a central position. Nice to have central pixel so you can talk about center of filter. 3x3, 5x5, 7x7 all\n",
    "        common. \n",
    "<img src=\"cnn_pad2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN strides</h6>\n",
    "For a 7x7 image if we move 2 pixels to right on each convolution and 2 pixels down after the horizontal direction\n",
    "then teh size with a 3x3 filter is given as $floor(\\frac{n+2p-f}{s}+1)$. For a 7x7 image, a 3x3 filter and stride 2 we\n",
    "have a final size of (7+0-3)/2+ 1 = (4)/2+1 = 3. This isa 3x3 output image. \n",
    "<img src=\"cnn_stride1.png\">\n",
    "Try some examples\n",
    "<img src=\"cnn_stride2.png\">\n",
    "Inconsistency in notation: because we arent mirror flipping the kernel/filter then this isnt strictly a convolution\n",
    "    opertaion but a cross correlation. Most of teh DL literature calls this convolution. But it really is not. IN\n",
    "    signal processign doing the flipping causes teh convolution to be associativity, (A*B)*C = A*(B*C) for NN this \n",
    "    doesnt matter. \n",
    "<img src=\"cnn_stride3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN Convolution over volume</h6>\n",
    "In a RGB image you use a sliding cube and the first convolution number is the sum of all 27 cells where you do one 3x3\n",
    "convolution on the G plane, then R, then B and add all together into a single number! \n",
    "<img src=\"cnn_vol1.png\">\n",
    "examples of filters, first one is a vertical edge detector for R channel only, secodn one is a vertical edge for all colors\n",
    "and 3x3x3 convolution gives 4x4. \n",
    "<img src=\"cnn_vol2.png\">\n",
    "what if we want both vertical and horizontal edges? how to use multiple fitlers at the same time? You can keep adding filters\n",
    "ie first vertical edges then horizontal edges to get 2 4x4 and put one in front and another back, 2 stacked 4x4x2 output volume\n",
    "where the first is vertical edges and the second is horizontal edges! \n",
    "<img src=\"cnn_vol3.png\">\n",
    "if you have input nxnxnc convolve to fxfxfc by convetnion fc=nc you get (n-f+1)x(n-f+1) x nc output.. no padding or strides\n",
    "you can adjust You can detect many features!!! Keep on stacking layers. \n",
    "<img src=\"cnn_vol4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN 1 layer</h6>\n",
    "Add biast to each of these layer, then apply a nonlinearity and bias. Add bias to each of 16. The convoluiton is similar\n",
    "to WA and the bias and nonlinearity g(Z)=a becomes activation for next layer. Convolution is like linear operation. \n",
    "gone from 6x6x3 and 3x3x3x2 to 4x4x2. Fi we had 10 filters instead of 2 we would have 4x4x10.  \n",
    "<img src=\"cnn_one1.png\">\n",
    "10 filters 3x3x3 filters w/10 filters, 280 parameters. One nice thing regardless of size of input images teh number\n",
    "of parameters stays teh same, use them to detect vertical edges, horizontal edges, etc..w/small number of parameters. \n",
    "Makes them less prone to overfitting. \n",
    "<img src=\"cnn_one2.png\">\n",
    "notation for single convolution layer, $f^{[l]}=filter layer$ and $p^{[l]}$ where valid means valid convolution and\n",
    "there is no padding and same means the padding is set so the output size after padding is same as input size. $s^[l]$ for stride\n",
    "input is nxnxnumber of channels modify the notation $n^{[l-1]}_hxn^{[l-1]}_wxn_c{[l-1]}$ and the size of teh outpue\n",
    "is ...the number of parameters change w/batch gradient descent.!!! MORE DETAILS>>.\n",
    "<img src=\"cnn_one3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN Simple CNN</h6>\n",
    "Example if volume w/input 39x39x3 then for layer1 we apply w/stride=1, no pad, f=3x3, floor((n+2p-f)/s)+1=37. Anotehr layer, L2, \n",
    "no pad, 20 filters, s=2. f=5. 17x17x20 output volume. Then a 3rd layer f=5, s=2, 40 filters. \n",
    "note bc u used stride2 the dimension shrunk much more rapidly. 7X7x40 is 1960. The resolution went down but the depth\n",
    "increased. \n",
    "For the last layer we can unroll into 1960 units into a softmax. Last step takes all 1960 numbers and unroll into long vector\n",
    "and feed into softmax. \n",
    "<img src=\"cnn_sim1.png\">\n",
    "3 types of layers in convnet. convolution layer. there are 2 other layers, pooling and FC layer. \n",
    "<img src=\"cnn_sim2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN Pooling</h6>\n",
    "Max pooling vs. avg pooling,pooling makes features more robust. max pooling takes max in grid. 4x4 input into 2x2 w/max \n",
    "pooling, take 4 subergions and take max. Pooling same as using filter 2x2, f=2 and s=2. These are hyperparameters\n",
    "of pooling. The intuition is if you think 4x4 as a set of features, lets say the 9 represents an eye. The max perserves\n",
    "this max feature. IF this feature is anywhere in this filter keep it. If there is no feature like the 2,1,1,. Found\n",
    "ina lot of experiments it works well. People dont really know if that is the real underlying reason it works\n",
    "well in convnets. maxpooling has hyperparameters there are no parameters for gradient descent ot learn. Or take avg. \n",
    "<img src=\"cnn_pool1.png\">\n",
    "examples of max pooling withe different hyperparameters. USe same formula to compute max pooling final size. floor(n-f+2p/s)\n",
    "+1. 5x5 f=3, s=1, ouput size is 3x3xnc. max pooling usually does not using padding. There is 1 exeption. \n",
    "<img src=\"cnn_pool2.png\">\n",
    "<img src=\"cnn_pool3.png\">\n",
    "<img src=\"cnn_pool4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>CNN example</h6>\n",
    "LeNet example. doing hand digit recognition. Build NN to do this. similar to lenet5. First layer uses 5x5 filter and s-1. \n",
    "use 6 filters so you get 28x28x6 for conv1. Maybe relu nonlinearity. then maxpooling layer, f=2.s=2. 2x2 filter. 14x14x6\n",
    "pool1 output. turns out there are 2 conventions. layer1 of NN another convetnion is conv layer as layer and pool as a layer.\n",
    "usually record layers as layers with weights. he uses conv1 and pool1 as layer1. sometimes people call this separate layers.\n",
    "\n",
    "<img src=\"cnn_ex1.png\">\n",
    "<img src=\"cnn_ex2.png\">\n",
    "<img src=\"cnn_ex3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>CNN Why Convolutions</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cnnq1.png\">\n",
    "not 45 degrees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a= np.array([[[0,1,-1,0]],[1,3,-3,-1],[1,3,-3,-1],[0,1,-1,0]])\n",
    "v=np.array([[0,30,30,0],[0,30,30,0],[0,30,30,0],[0,30,30,0]])\n",
    "h=np.array([[0,0,0,0],[30,30,30,30],[30,30,30,30],[0,0,0,0]])\n",
    "f5=np.array([[30,30,30,30],[0,30,30,30],[0,0,30,30],[0,0,0,30]])\n",
    "#print(np.multiply(a,v))\n",
    "print(a*h)\n",
    "#print(a*f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep CNN Case Studies</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6><Week2 Deep CNN Classic Networks /h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Deep CNN RseNets </h6>\n",
    "To get around vanishing/exploding gradients in deep networks, add a short cut layer. Original path is $a^{[l+2]}=g(z^{l+2})\n",
    "$which disappears and is replaced by the $a^{[l+1]}=g(Z^{[l+2]}+a^{l})$ where g is a nonlinearity, relu. the al term is the \n",
    "residual. Teh shortcut is added before the relu part. resntes allows you to train much deeper. \n",
    "<img src=\"res1.png\">\n",
    "this is a plain network. to turn into resnet add skip connections to teh plain network. plain has no skip connectins.\n",
    "if you use a standard optimization to train a plain network you see as you increase number of layers the Training error\n",
    "goes down then goes up. In theory the deeper network should only help. IN practicd this isnt the case. IN reality\n",
    "the training error increases! wiht resnets the training error keeps on going down. But the error is higher/per layer.\n",
    "<img src=\"res2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Deep CNN Why Rsnets work</h6>\n",
    "intuition of why resnets work so well. If you apply L2 regularization that will be weight decay to W that will shrnk W\n",
    "or if you apply weight decay to b but in practice sometimes you do and sometimes you dont. \n",
    "<img src=\"res3.png\">\n",
    "The shapes of the terms Z[l+2] and a[l] have to be the same so you use conv[same]\n",
    "<img src=\"res4.png\">\n",
    "You can adjsust the a[l] term by putting weights there or learnable parameters(exercise)\n",
    "<img src=\"res5.png\">\n",
    "<img src=\"res6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep CNN Newtorks in Networks and 1x1 conv</h6>\n",
    "What is the use of a 1x1 conv? You can reduce the nc dimension. This is useful for speeding up processing or making \n",
    "problems computationally tractable. \n",
    "<img src=\"cnn_one1.png\">\n",
    "<img src=\"cnn_one2.png\">\n",
    "<img src=\"cnn_one3.png\">\n",
    "<img src=\"cnn_one4.png\">\n",
    "<img src=\"cnn_one5.png\">\n",
    "<img src=\"cnn_one6.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week2 Deep CNN Inception motivation </h6>\n",
    "What happens when you concatenate different layers with differet attributes like maxpool? etc..\n",
    "<img src=\"cnn_incep1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep CNN Inception network</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep CNN open source</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep CNN transfer learning</h6>\n",
    "<img src=\"cnn_trans1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep CNN data augmentation</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Deep CNN state of CV</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week 3Object Localization</h6>\n",
    "Notation is $p_c$ is probability of object origin upper left hand corner, bh,bw is width and height of object relative\n",
    "to image size, and bx and by is location of center of object in fractions of image size. Center of trucks is (.5,.7) where\n",
    "bx is .5 or x coordinte is half image over and y coordinate of image center is .7 of image height. \n",
    "<img src=\"objlocal1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week3 Landmark Detection  </h6>\n",
    "Landmark is just outputting x,y coordinates of object. Manually annotate images with landmark. And the convnet\n",
    "will output 0/1 if there is a face or not in addition to other coordinates. Example is if there is a face or not and 64 landmark\n",
    "ccoordinats which have x,y position. Output is true/false on face then 64*2 coordiantes = 129 total output neurons. \n",
    "Second exmaple pose detection, annotate pose coordinates on different perspective views of people.\n",
    "<img src=\"landmark1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week3 Object Detection  </h6>\n",
    "Start w/hand labeled images of cropped cars as training set and train convnet. The car is the object. Does the \n",
    "image contain a car? PRocess the entire image. \n",
    "<img src=\"car1a.png\">\n",
    "Start in upper left hand cornder and run sliding window of 3 different sizes and feed this into convnet to see \n",
    "if there is a car or not. This is a fully connected convnet implementation. \n",
    "<img src=\"car2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week3  Conv Implementation of Sliding Windows</h6>\n",
    "We can turn FC layers into convolutional layers. how to implement a sliding window in a convolution? Modify te softmax\n",
    "picture to 4 classes corresponding to 4 probabilities. Start w/an object detector algorithm with 14x14x3 images and 16 5x5 \n",
    "filters to map into 10x10x16 output volume then 2x2 maxpool which reduces to 5x5x16 output volume then 2 FC layers\n",
    "and a softmax output. \n",
    "\n",
    "You can convert the FC layer into a CONV layer! The input, 14x14x3 is teh input image. This is the input to the first 5x5x3 conv layer producing\n",
    "and output volume of 10x10x16. Use the same 5x5 filter but 400\n",
    "of them!! So how deep is this? it has to be 400 deep but each 5x5 filter is now a 5x5x16 filter to preserve the input \n",
    "depth. NEED TO TEST THIS OUT....\n",
    "\n",
    "<img src=\"slide_conv1.png\">\n",
    "how to convert a sliding windwos. From OverFeat paper Integrated recognition localization using convolutional networks. \n",
    "\n",
    "\n",
    "<img src=\"slide_conv1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week3 Bounding Box predictions </h6>\n",
    "The problem is the sliding window doesnt exactly match the bounding box. Can get a close one but it depends on hwere\n",
    "the object is relative to the sliding window. YOLO fixes this problem. If have 100x100 image divide the image into grids\n",
    "Each grid gets an object lcoalization algoritm.\n",
    "<img src=\"bound1.png\">\n",
    "<li>Apply the naive image classification to each 8 grid cells. Define some convention first. \n",
    "Specify a label Y for each grid cell. y is a 8 dim vector. [pc,bx,by,bh,bw,c1,c2,c3] to specify teh bounding box if there is \n",
    "an object associated w/grid cell. pc is probability of an object, c1,c2,c3 is the class of hte object where we have\n",
    "3 classes and bx,by,w,h is the bounding box we have from beore. </li>\n",
    "<li>We have 9 grid cells and a y vector for each grid cell. Upper left there is no object\n",
    "so the label vector Y is 0 and dont cares for the rest. This is true for all grid cells w/no object. [0,?,?,?,?,?,?,?]</li> \n",
    "The yolo algo takes the left car and right car and assigns the cars to a grid cell. \n",
    "The central grid cell has y=[0,????????]. The left vector\n",
    "is y=[1,bx,by,bh,bw,0,1,0]. there are labeled bounding boxes for these. For each of the 9 bounding boxes you have a 3x3x8 dim vector. \n",
    "To train the NN the input is 100x100x3. We break up the image into 9 grids. Add padding to make a multiple of \n",
    "teh filter size or along the edges. He uses 3x3 but can do 19x19 much fineer to reduce prob multiple objects in 1 grid.\n",
    "As reminder: you assign midpoint of object to grid. Assign object to 1 gridcell which contains the grid. Even if object\n",
    "    spans multiple cells. \n",
    "<img src=\"bound2.png\">\n",
    "Looks like object detetion which outputs bounding boxes we saw wearlier. This looks like an object detection. This is a \n",
    "convolutional implementation. Not implmeenting 9 times. Or 19x19 not running same algo 19x19 times. WORK OUT MATH FOR 19x19 in 100x100\n",
    "Once nice thing about yolo is runs fast for real time. One more detail: how do you encode the bounding boxes? \n",
    "<img src=\"bound3.png\">\n",
    "On orange object on right, how do  you specify bounding box? Relative to square upper left is 0,9 and lower right is 1,1\n",
    "bx looks like 0.4 to  right and by is 0.3. Width maybe 90% and height is 1/1 of total \n",
    "<img src=\"bound4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week3 intersection over union </h6>\n",
    "how to tell if object detection is working well? In object detection you want to localize. The purple is what you get\n",
    "from convnet. The metric is intersection/union, green = union, orange=intersection. if iou>0.5 then you are good. \n",
    "0.5 is human chosen convention. 0.6 is more stringent. Rarely see less than 0.5. What motivates this? Also a way to measure \n",
    "how similar 2 boxes are. \n",
    "<img src=\"iou1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week3 Non max suppression </h6>\n",
    "nonmax suppression to make suer you detect each object only once. \n",
    "<img src=\"nonmax1.png\">\n",
    "Find highest probability and suppress the bounding boxes with highIOU. So you end up 2!. Output the maximal prob and\n",
    "supress the non max ones. \n",
    "<img src=\"nonmax2.png\">\n",
    "on 19x19 grid you will get 19x19x8 output volume. you only doing car detection and get rid of c1,c2,c3. You get\n",
    "an output of pc and a bounding box. First step: dicsard all bounding boxes w/pc<0.6. Pick a threshold. For each of the\n",
    "    361 position you output a bounding box and pc which is probability of it having object. Discard low pc. For the ones that\n",
    "    are left, find the highest pc and make this prediction. Discard any remaining box with IOU>0.5 from the step before. \n",
    "<img src=\"nonmax3.png\">\n",
    "The above is for 1 object so we got rid of c1,c2,c3. IF there are multiple objects run this one time for each class of objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week3 anchor boxes </h6>\n",
    "Problem is can only detect 1 object. What to do? Note for grid cell we have person and car in same cell!!. What to do?\n",
    "Anchor boxes predefines 2 different shapes, associate 2 predicitons w/2 anchor boxes. Can do 5 or more anchor boxes. The\n",
    "vector is a series of anchor boxes. Put the center of each anchor box into the center of each grid. So you search\n",
    "inside the anchor box inside the grid. \n",
    "<img src=\"anchor1.png\">\n",
    "\n",
    "<img src=\"anchor2.png\">\n",
    "<img src=\"anchor3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week3 YOLO </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week3 Region Proposals </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week4 what is face recognition  </h6>\n",
    "Face verification: input an image and compare vs. stored image 1:1 problem. Compare 1 input to 1 stored image. \n",
    "State of art get 99%. Face recognition: compare vs. K persons and output to one of K. Much harder. Start w building a\n",
    "    Face verification system first and if the accuracy is good enough you can use it for facial recognition\n",
    "    <img src=\"facever1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week4 one shot learning  </h6>\n",
    "Difficulty is 1 training image for recognizing person. One system is input face into cnn and output a softmax. This doesnt\n",
    "work well. Not enough data to train CNN. You get 5 outputs if there 4 people/1 is nobody. \n",
    "Learn a similarity funciton instead. If distance is  less than some threhold tau is not similar, >tau is similary. \n",
    "Use this funciton d to compare these 2 imsages. \n",
    "<img src=\"oneshot1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week4 siamese netowrk  </h6>\n",
    "NN to train this function d w/siamese network. INput photo to CNN and remove the last softmax layer. We want the \n",
    "deeper 128 neuron layer as the output. Representing the image as a vector of 128 numbers. A FC layer. You feed the \n",
    "second  piccture to same NN w/same parameters and get a different vector of 128 numbers. f(X(1)) is the encoding if photo\n",
    "X(1). You can use these encodings to determine the distance between the encodings between these 2 images. \n",
    "<img src=\"siam1.png\">\n",
    "how to you train the siam NN? the parameters of NN define an encoding f(x(i)). learn parametres so if 2 photos are fo same\n",
    "person then you want the distnance to be small. Use bp to minimze the error. Looks like a form of triplet loss. \n",
    "<img src=\"siam2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Week4 triplet loss  </h6>\n",
    "Want the distance from anchor to positive to be less than distance from anchor to negative. Then add the alpha for margin\n",
    "Supposed to derice this from basic principles. \n",
    "<img src=\"triplet1.png\">\n",
    "Example if margin is .2 and A-P=0.5 and A-N is .51 then that is not enough we want a larger difference. \n",
    "<img src=\"triplet2.png\">\n",
    "Loss function, defined on triples of images\n",
    "<img src=\"triplet3.png\">\n",
    "<img src=\"triplet4.png\">\n",
    "<img src=\"triplet5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week4 face verification and binary classificatoin  </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week4 whatr is neural style transfer  </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week4 waht are deep convnets learning  </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week4 cost function  </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week4 content cost function  </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week4 style cost function  </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week4 1d and 3d generalizations  </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Quiz1 CNN</h6>\n",
    "<img src=\"cnnq1.png\">\n",
    "<img src=\"cnnq2.png\">\n",
    "<img src=\"cnnq3.png\">\n",
    "<img src=\"cnnq4.png\">\n",
    "<img src=\"cnnq5.png\">\n",
    "<img src=\"cnnq6.png\">\n",
    "<img src=\"cnnq7.png\">\n",
    "<img src=\"cnnq8.png\">\n",
    "<img src=\"cnnq9.png\">\n",
    "<img src=\"cnnq10.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Quiz2 CNN</h6>\n",
    "<img src=\"cnn_q2_12.png\">\n",
    "<img src=\"cnn_q2_34.png\">\n",
    "<img src=\"cnn_q2_56.png\">\n",
    "<img src=\"cnnq2_78.png\">\n",
    "<img src=\"cnn_q2_9.png\">\n",
    "<img src=\"cnn_q2_10.png\">\n",
    "<img src=\"cnn_q29wrong.png\">\n",
    "<img src=\"cnn_q24wrong.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"quizface1_4.png\">\n",
    "<img src=\"quizface5_8.png\">\n",
    "<img src=\"quizface9_10.png\">\n",
    "<img src=\"quizface_graded1_5.png\">\n",
    "<img src=\"quizface_graded6_10.png\">\n",
    "<img src=\"quiz2_1_3.png\">\n",
    "<img src=\"quiz2_5.png\">\n",
    "<img src=\"quiz2_4_5.png\">\n",
    "<img src=\"quiz2_6_8.png\">\n",
    "<img src=\"quiz2_9_10.png\">\n",
    "<img src=\"quizface2_graded1_6.png\">\n",
    "<img src=\"quizface2_graded7_10.png\">\n",
    "<img src=\"quizface2_graded_7_9.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
