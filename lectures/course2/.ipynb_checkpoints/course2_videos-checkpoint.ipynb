{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>review questions</h6>\n",
    "<img src=\"c2q1.png\">\n",
    "Only need 10k test or dev samples because the distibution is the same. Wont be more accurate if data is iid\n",
    "and you are sampling correctly. \n",
    "<img src=\"c2q2.png\">\n",
    "The dev and test have be from the same distribution. The training set can be from a different distribution. WHen \n",
    "training you are generalizing to the dev set. If you generalize to the dev set and the test set is from a different\n",
    "distribution you will either underfit or overfit. (VERIFY)\n",
    "<img src=\"c2q3.png\">\n",
    "For high bias you are underfitting so you need to make the network bigger, \n",
    "For high variance you are overfitting so you need to make the network smaller like with dropout or regularization. \n",
    "<img src=\"c2q4.png\">\n",
    "If the dev error is more than the training error then we are overfit. If overfit then increase regularization and \n",
    "get more training data. \n",
    "<img src=\"c2q5.png\">\n",
    "weight decay is regularizatio. DERIVE!!!\n",
    "<img src=\"c2q6.png\">\n",
    "weights pushed to 0. \n",
    "<img src=\"c2q7.png\">\n",
    "Test time you do nothing. You can randomly drop out neurons and then average the results but is the same thing \n",
    "more or less. \n",
    "<img src=\"c2q8.png\">\n",
    "tricky, dropping out more neurons decreasing regularization and decreases training set error. \n",
    "<img src=\"c2q9.png\">\n",
    "to reduce variance, L2 regularization, dropout. Early stopping, data augmentation, get more training data. \n",
    "<img src=\"c2q10.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Train dev test set</h6>\n",
    "<img src=\"course2_l1a.png\">\n",
    "the intuition from 1 domain dont transfer to another ie NLP to/from computer vision. Train/dev/test dataset. Take\n",
    "total data set divide into train/holdout(dev)/test set. Train and test for error on dev set. Which model performs best on\n",
    "dev set then measure final model on test set. Common to do 70/30 or 60/20/20. This was best practice for 100/1000/10k \n",
    "exampels. Modern big data 1M train/dev smaller percentage of total. dev only big enough to evaluate algorithm set. 10k\n",
    "examples for dev set maybe enough. 10k for test and 10k for dev so 98/1/1 where test=10k and dev=10k. rest for training\n",
    "if total is 1M. \n",
    "<img src=\"course2_l1b.png\">\n",
    "Make sure the dev and test sets come from the same distribuiton. To get larger datasets for training we crawl web\n",
    "for cat pics then we use the app to deliver test and dev cat images. these come from 2 different distributions. Never\n",
    "mix the dev distribuiton w/training distribution at risk of overfitting. Having no test set and only having dev set is\n",
    "ok if you dont need unbiased measure of error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Bias and variance</h6>\n",
    "<img src=\"course2_l2a.png\">\n",
    "In DL there is less bias/veriance tradeoff.high Bias=underfit,high variance=overfitting. \n",
    "<img src=\"course2_l2b.png\">\n",
    "Easy to visualize in 2d but \n",
    "for higher dims you cant do this. Instead use test/dev error. if train error is 1% but dev error is 11%. This is overfit\n",
    "bc you arent generalizing well. High variance. If you measure train error 15% and dev error 16%. Assuming humans get\n",
    "0% error then this is underfitting or high bias. IF 15% training error then dev set is 30% this has high variance and high bias\n",
    "if you have 0.5% train error and 1% dev error. This is based on humans is 0% or bayes error is 0%. This is optimal error\n",
    "if the optimal error is 15% then the second case is best, train is 15% adn dev is 16%. Looking at training error you \n",
    "can tell if bias error and loking at if error goes up or changes in dev set then that givve you variance. \n",
    "<img src=\"course2_l2c.png\">\n",
    "With high dimensionality you can get both high bias AND high variance in the same model!!! in the 2d example if we get\n",
    "a model which is the purple line that is high bias bc it underfits the 2 points 1 red which shouldnt be thre and\n",
    "one black dot. But w/a DL model we can get a decision surface which curves round those 2 points giving us a perfect fit\n",
    "this ia overift or high variance. Se we get both!!! Holy shit. THis is more common in high dimensional data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Basic Recipe</h6>\n",
    "<img src=\"course2_l2d.png\">\n",
    "\n",
    "First ask if high bias? IF so try bigger network, train longer, or different NN architecture. Try until get rid of bias \n",
    "problem. Big enough network should be able to fit data. If bayes error is not too high. Once bias reduced ask if you\n",
    "have variance progble. Look at dev error. SOlve variance problem by regularization to reduce OF or more data. OR get new\n",
    "NN architecture. Keep on going back to reduce the 2 errors. If you have high bias problem getting more data wont help. \n",
    "IN earlier DL would talk about bias variance tradeoff. Reduce bias then increase variance. IN modern times this \n",
    "rule isnt true. Back in pre deep learning era didnt have tools to just reduce bias or variance wo hurting the oher one. \n",
    "We have that now!! Getting bigger network reduces bias wo hurting variance as long as you regularize. Getting more\n",
    "data reduces variance doesnt hurt bias. we have tools tu just drive down bias or drive down variance. Thsi is one of \n",
    "teh big reasons DL is so useful for supervised learning. training a bigger network almost enver hurts unless inference time\n",
    "is issue, regularization useful tech for reducing variance. Might increase bias little bit but not as much if you have large\n",
    "network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Regularization</h6>\n",
    "<img src=\"course2_l3a.png\">\n",
    "If you have high variance or overfitting add regularization. Look at Logistic Regression. Loss function J\n",
    "L2 used more often than L1. L1 produces sparse weights which some say can compress model. Ng says makes small difference not really useful. Lambda another hyperparamter, set using dev set or use holdout cross validation. Try values and see what works best. This is how you implement regularization for logistic regression=. how about for DL? \n",
    "\n",
    "<img src=\"course2_l3b.png\">\n",
    "NN you have Loss function function of all parameters, cost function is plus regularization term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Why Regularization reduces overfitting</h6>\n",
    "There are 2 interpretations of reducing overfitting or variance in NNs.\n",
    "<img src=\"course2_regoverfita.png\">\n",
    "Reduce teh complexity of teh model by setting the weights closer to 0. Why is it a frobenius norm which reduces\n",
    "the weights will reduce variance? Subtle cause this isn't the same as just making the weights smaller.\n",
    "Think of weight decay as a function. Says keeping more weights closer to 0 means this is a smaller network and\n",
    "this is like a layerd logistic regression network. q) what does the derivation of a stacked LR network look\n",
    "like compared to a deep network w L2 regularization? compare the math equations. \n",
    "\n",
    "<img src=\"course2_regoverfitb.png\">\n",
    "Be careful to plot the loss w/teh regularizer term not teh loss wo the regularier. You may not see teh loss decrease\n",
    "like it does w/o regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Dropout REgularization</h6>\n",
    "<img src=\"dropout1.png\">\n",
    "Remove the dropout factor number of nodes at each layer. \n",
    "<img src=\"dropout2.png\">\n",
    "How to implement dropout. Inverted dropout. You have to divide by keep-prob to restore expected value. Makes test time \n",
    "easeier. At test time you need th ave divide by keep-prob to make it easier!!!!!!\n",
    "Different passes through teh training set you should zero otu different hidden units. How does reducing the network\n",
    "size reduce the overfitting? \n",
    "<img src=\"dropout3.png\">\n",
    "what to do at test time? dont use dropout at test time. IN theory You can randomly dropout nodes for each test example and average out \n",
    "between them that should give the same result but that\n",
    "is too computationally ezpensive. Dont use dropout at test time. Same qquations as wo dropout. He did nothing. \n",
    "At test time no dropout. Dont want output to be random. \n",
    "\n",
    "<img src=\"dropout5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Unerstanding dropout</h6>\n",
    "<img src=\"under_drop1.png\">\n",
    "Why does it work so well as a regularizer? As if you are working w/a smaller neural network. a smaller network should\n",
    "have a regularizing effect. W dropout the inputs and get randomly eliminated. Teh purple neoron cant rely on any feature \n",
    "so you have to sperad out weights to eacgh of the other input neurons. This shrinks the weight value. Similar to L2 regulazrization\n",
    "Droppout can be shown to be formally to be an adaptive form of L2 regularization but the L2 penalty on different weights are different. \n",
    "How!!!!????? This seems like it would be better than standard L2 regularizaton. HOW example????\n",
    "<p></p>\n",
    "You can set different dropouts for different layers. You can put higher dropout on the bigger layers. The downside\n",
    "is you have more hyperparameters to search for if every layer has a dropout. \n",
    "What happens if L2 regularization is too high? You will see underfitting. How do you measure this? Ny the dev/training error. \n",
    "Dropout good for high variance. In computer vision you dont have enough data so you are almost always overfitting\n",
    "and it has more use. Doesnt always generalize to other domains. Downside is teh cost function J is no longer well defined\n",
    "Every iteration you can match gradient descent with the decrasing loss graph. You lose this debugging tool if you \n",
    "use dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Other regularization methods</h6>\n",
    "<img src=\"augment.png\">\n",
    "\n",
    "<img src=\"earlystop.png\">\n",
    "plot train error and dev set error. stop before dev set error goes up. ES similar to reducing magnitude of weights.\n",
    "As you train more weights increase in magnitude, the Downside: think of ML process as individual steps, frist loewer loss\n",
    "    then next step is to fix overfitting .this is orthogonilization of tasks. So there are 2 sets of tools, one set for \n",
    "    reducing J like RMSprop, adam, and anothet set of tools for reducing overfitting like regularization. DOwnside is ES\n",
    "    couples these 2 tasks so you can no longer work on these 2 problems independenty. By stopping gradient descent early you\n",
    "    can longer work on making it better. One laterntive is to use l2 reglylarization. doenside: try lots of lamnda. one benefit\n",
    "        is you can work on other parametesrs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Normalizing inputs</h6>\n",
    "<img src=\"c2w2normal1.png\">\n",
    "one of the techniques to speed up training is to normalize inputs. center at 0 by subtracting the mean and divide\n",
    "by variance. This shifts teh data to be centered around the origin with a unit radius. Why does this work? \n",
    "<img src=\"c2w2normal2.png\">\n",
    "This makes the gradient descent converge faster. The 2d bowl converges faster than the squashed ellipse. DOES THIS \n",
    "REMOVE OR ADD LOCAL MINIMA? No, but it may spread them out? Make sure to apply this to test/dev set when testing, subract\n",
    "mean and divide by variance. This guarantees all the input features are all of teh same scale. If the input comes\n",
    "in on similar scales then it shouldnt matter but it does no harm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Vanishing and exploding gradients</h6>\n",
    "<img src=\"c2w2grad1.png\">\n",
    "When weights  bigger than 1 then w deep network the activations will explode. And are less than 1 then the activations will vanish.\n",
    "The gradients will increase or decrease exponentially you can show thiS!!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Weight initialization for DN</h6>\n",
    "<img src=\"c2w2init.png\">\n",
    "A sngle neuron example chaining gradients.4 input features, some activation. z=WX where b=0. To make Z not blow and\n",
    "not be too small. Then for large n you want smaller w. One thing you can do is set variance of wi to be 1/n. \n",
    "A partial solution to exploding/vanishing gradients are to manage teh initialization of network. single neuron example: \n",
    "Keep variabce 1/n. An init the weights to random *scaling factor where sf = sqrt(2/n^[L-1]).Fro relu use 2/n for variance.\n",
    "If using tanh ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Numerical Approzimation of gradients</h6>\n",
    "<img src=\"c2w2check.png\">\n",
    "How to approximate checking of gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Gradient checking</h6>\n",
    "<img src=\"c2w2gradcheck1.png\">\n",
    "Put W,b into one big vector theta. contatentate them together. And dW, db into one big vector dTheta. \n",
    "<img src=\"c2w2gradcheck2.png\">\n",
    "How to tell if 2 vectors are same? Use L2 norm, sum of squares of difference hten square root. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Gradient checking implementaion notes</h6>\n",
    "Slows down computation. Only on during debug. Dont forget regularization term. \n",
    "Doesnt work w dropout. \n",
    "<img src=\"c2w2gradimpl1.png\">\n",
    "Not impossible but grad can be correct when W and b are close to 0. But can be inaccurate when W and b are off when large. \n",
    "Can run gradnetwork at random init, then let netowk train for some time then run grad check again after large number of iteration\n",
    "å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Multibatch gradient descent</h6>\n",
    "Notation: use the brackets to denote minibatch\n",
    "<img src=\"c2w2_mini.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Understanding minibatch gradient descent</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Exponentially weighted averages</h6>\n",
    "To compute the moving average say we want .9 of todays temp and .1 of yesterdays temp. If this is a moving\n",
    "average it becomes exponential because writing out the series shows a $.9^n$ and for 10 days we see it relate to \n",
    "1/e or 1/3\n",
    "<img src=\"c2w2_exp1.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Understanding exponentially weighted averages</h6>\n",
    "For 10 days we see the line approximages teh blue data points but 50 days moves the weighted average to teh right\n",
    "and shows a time delay in the values. Can argue it doesnt change fast enough. Is smoother though. Is smootehr better \n",
    "for sparse where sparse are for NLP applicatiions where you have 1 hot. The one hot is replaced w/embedding, \n",
    "is this still sparse? L1 is sparse. \n",
    "<img src=\"c2w2_exp2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Bias corretion in exponentially weighted averages</h6>\n",
    "Problem in weighted averages is the start of the weighted average is too low because we start at 0 and there\n",
    "is no weighted average history. One way to correct is to divide by $1/(1-{\\beta}^t)$\n",
    "<img src=\"c2w2_bias.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Gradient descent w momentum</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>RMS Prop</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Adam Optimization</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>learnging rate decay</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Problem of local optima</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 Tuning Process</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 using an appropiate scale to pi ck hyperparameters</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 hyperparameters tuning in practie?</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 normalizing activations in a network</h6>\n",
    "Normalize the inputs at each stage to speed up training. Can you predict how fast this is? \n",
    "Same as normalizing input subtract mean to center at 0 and divice by variance. Q) what does this really do? \n",
    "normalizes ranges to unit circle. Debate about where to normalize, at Z. The mean and variance are controlled by gamma and beta. \n",
    "<img src=\"c2w2batch1.png\">\n",
    "Addnymerical exampe\n",
    "<img src=\"c2w2batch1a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Week2 fitting batch norm into a NN</h6>\n",
    "In the 2 stage NN below there rae 2 places where we add BN, calculate $\\beta^{[1]}$ adnd $\\gamma^{[1]}$. The batch norm\n",
    "step occurs between Z and the activation. Compute $\\tilde{Z}$\n",
    "Batch norm \n",
    "<img src=\"c2w2batchnorm1.png\">\n",
    "<b>Working with minibatches</b>\n",
    "BatchNorm up to now we have been talking about the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>PRoblems</h6>\n",
    "<h6>BP through BP<h6>\n",
    "<h6>Add other functions, a diice by 2. or sqrt. how to probagete this? </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 Why does batch norm work?</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 Batch norm at test time</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 softmax regression</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 training a softmax classifier</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 deep learnign frameworks</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 tensorflow</h6>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
