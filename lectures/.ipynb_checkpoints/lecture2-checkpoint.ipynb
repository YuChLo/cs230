{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"l2a.png\">\n",
    "If we want multiple classes instead of 1 class in logistic regression what modifications do\n",
    "we add? We add 1 neuron for each new class. E.g. dog is a new class. \n",
    "<img src=\"l2b.png\">\n",
    "Add a neuron to support class\n",
    "dog. The lasy layer input into the sigmoid is the number of classes. What will tell the model \n",
    "one neuron is cat one is dog? Teh labels. HOw to lable the data? One hot vector. Or  use integers\n",
    "to map. Modfiy teh loss function, put more weight on 1 animal. Downside to 1 hot encoding, mostly sparse\n",
    "waste of space. $a^{[2]}$ the square brackets denote an activation. If you have a network that is\n",
    "not too shallow. The rounded brackets denote the training set $x^{(1)}$ is the first training example. \n",
    "<img src=\"l2c.png\">\n",
    "The first neurons represent each image at the pixel level(window) then the hidden layer represesnts edges\n",
    "assuming there is a convolution. Even if there is no convolution and this ia a feedfoward network\n",
    "you will see edges or some higher level features. \n",
    "<img src=\"l2d.png\">\n",
    "Day and Night classificaiton. What kind of data? Images taken at day and night. \n",
    "How many images do you need? For the cat classifier we used 10k images. This task looks\n",
    "less complicated than the cat problem so you probably need less data. Split maybe 80/20 if big dataset can go 90/10\n",
    "or 98/2 if big enoug. Bias means bias against classes. Input: pixel image. Resolution: as low as you can to achieve \n",
    "good results bc have more cost/computation for more pixels. What is good performance? Use human performance as comparison\n",
    "or do brute force and compare resolutions which is slower. So for human he get 64x64x3 to compare w/human to distinguish\n",
    "day/night or cat. Output: ouput is y=1 or 1, day or night. Last actiation: sigmoid. Architecture: fully connected or convolutional.\n",
    "Loss function: logistic for 2 class. MLE estimateion take highest probability. That was the warmup exanple. \n",
    "<img src=\"l2e.png\">\n",
    "How to design\n",
    "the facial verification: data:images, 400x400x3 collect reference images and labels \n",
    "input:facial images  output: name/verify to name/boolean true/false,architecture: what is the easiest way to \n",
    "            compare 2 images? a hash. the more basic is difference of images. Subtract 2 pixels. Doesnt work to compare \n",
    "            the 2 pictures together. Need function to compare these 2. A vector to represent features like eyes, hair, etc.. A deep nerwork. This\n",
    "            training process is like embeddings. HOw do you know this vector is good? Loss? training? we need more data. university w 1000 images, we \n",
    "            need millions of faces and use this model to train the model. Could use n classifier for n students but each student is a separate class but\n",
    "            you have to retrain every year when you get new students. generate triplests.  \n",
    "activation:\n",
    "<img src=\"l2f.png\">\n",
    "Loss and training. Notice how you are generating triplets and comparing against each other!! Very useful strategy. \n",
    "Minimize encoding distance between 2 similar faces and maximize against another one. Separate by gender. \n",
    "<img src=\"l2g.png\">\n",
    "We want to minimize A-P but maximize A-N. B loss is max - min A loss is min - max. B you max wheras A is min. A is minimize and B\n",
    "is maximize. We want the loss to go down. \n",
    "<img src=\"l2h.png\">\n",
    "After we run this w/the loss we should get an encoding of the face. \n",
    "<img src=\"l2i.png\">\n",
    "True loss contains an alpha. Why? Because 0 is a trivial solution of the loss. Add a term alpha to \n",
    "make sure 0 is not a solution. This term alpha is also called a margin. Margin pushes network to learn. \n",
    "Also has to do with initialization. Only saw 0 initialization. We can change initializaiton scheme to get rid of alpha.How do we\n",
    "know the network is robust to rotation? we can augment the input data set to scale, rotation, etc.. \n",
    "<img src=\"l2j.png\">\n",
    "Make problem harder? Want to recognize face in other areas of facility. You have features of all students. Get vector of \n",
    "all students, you enter facility and compare this vector vs. all the students in the database. The complexity is N. K nearest\n",
    "neighbors. Want to use face clustering which is kmeans; take all vectors in database can cluster. Run kmeans on all faces\n",
    "to see which ones are similar. \n",
    "<img src=\"l2k.png\">\n",
    "Art generation make something beautiful. Data: collate data defining beautiful. Take the context image and a style image\n",
    "    we want to generate an image which looks like ontent image but painted by style image. Architecture: deep network. Generative network. Get content\n",
    "        image and we have style network which will modify image. One method which we arent going to use. We aren't going to learn parameters, we are\n",
    "        going to backpropagate back to the image. Different. \n",
    "<img src=\"l2l.png\">\n",
    "This is tricky. He uses an imagenet model or you can train using an autoencoder/decoder. Instead we take the\n",
    "imagenet decoder/encoder. How do you know which one is better? \n",
    "<img src=\"l2m.png\">\n",
    "THe style loss is B we are addind content loss + style loss and minimizing the total. \n",
    "<img src=\"l2n.png\">\n",
    "downside is everytime you want to getnerate a new image you have to do loop back propagating. We start w/white noise, run it through imagenet. You get\n",
    "weights, Cg and styleG. Extract info and able to compute loas ufnction. BP to pixels and decide how much to change pixels\n",
    "which eventually converge to image. How to do you extract CG and styleG? network weights are not updated. You have CG and SG\n",
    "and update compared to a reference style which is monet or something else. Why do  you start w/white noise? Better t\n",
    "start w/content or style? Says content bc edges are there. Should be faster. CG and styleG are in the first few layers. Use gram matrix which is \n",
    "basically cross correlation of all features of network to get styleG. \n",
    "<img src=\"l2o.png\">\n",
    "Detect teh word accident w trigger word detection. Data? a lot or no? Yes a lot bc there are a lot of accents. \n",
    "<img src=\"l2p.png\">\n",
    "<img src=\"l2q.png\">\n",
    "What kind of loss function is this? Bounding box for YOLO. Why  is there square root? You want to penalize teh smaller \n",
    "errors on smaller bounding box, penalize aerrors more on small boxes than big boxes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
