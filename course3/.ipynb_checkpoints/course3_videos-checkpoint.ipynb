{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Why ML Strategy</h6>\n",
    "<img src=\"mlstrat1.png\">\n",
    "where do you put time to get better than 90% accuracy? Do you find more data? Do you \n",
    "make a better model? One person spent 6 months getting more data and it didn't improve\n",
    "performance of system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>orthogonilization</h6>\n",
    "<img src=\"mlstrat2.png\">\n",
    "The point is steering and speed are orthogonal to each other. Adjusting ML you want ortho controls when tuning hyperparameters\n",
    "So how is this relevant? We want the optimizing of the training error to give the same optimal result \n",
    "on the dev/test errors and we want variables we can adjust to keep dev/test error orthogonal to other metrics\n",
    "so we can adjust optimize dimensions independently without hurting other metrics when we adjust. \n",
    "<img src=\"mlstrat3.png\">\n",
    "<h6>Knobs for each part. </h6>\n",
    "<li>If the goals want to get to human level performance then the first step is to focus\n",
    "on reducing training set error. You want 1 knob to affect training error which should be independent\n",
    "of dev and test set errors. If you reduce teh training set error you dont want to impact test/dev errors. If\n",
    "not fittign well on cost function on training data. Which knob do you use to tune? \n",
    "If want improve train error, use bigger network or better optimizations like Adam opt(OTHER OPTOINS DISCUSS)</li>\n",
    "<li>After doing well on training error you hope you do well on dev set.\n",
    "to improve dev error only add regularizaiton or bigger training set. This increases the generalization between\n",
    "the training set and dev set. </li>\n",
    "<li>after doing well in train and dev you hope for good results on test set. \n",
    "if want improve test error but you do well on train and dev set and if test set is poor then BIGGER DEV SET. \n",
    "if good on dev set but not good on test then not generalizing well. increase the dev data set. </li> \n",
    "<li>IF not good in real world then change dev set or cost function. Either dev set not complete enough or \n",
    "loss function not optimizing right criteria. </li>\n",
    "<li>AN does not use early stopping bc it affects both train and dev error, ES is not orthogonal to the 4 errors. </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>single number evaluation</h6>\n",
    "<img src=\"mlstrat4.png\">\n",
    "Precision and Recall there is a tradeoff between the 2. the problem w/P and R if A does better on recall but B does better\n",
    "on precision then you arent sure which one is better. As you modifiy hyperparameters how do you know which is best so\n",
    "you can get rid of the others and iterate only on 1? Rather than using R and P. define a new metric to combine R and P. \n",
    "Like F1 score. \n",
    "$F1=\\frac{2}{\\frac{1}{P}+\\frac{1}{R}}$. Harmonic mean has some advantages. You can see A has a better F1 score\n",
    "than B. So select A!. \n",
    "<img src=\"mlstrat4a.png\">\n",
    "Another example is lets say you are building a CAT app to detect cats. \n",
    "IF you are building using US only data then you pick classifier A. Real world story of where this is an example of moving\n",
    "the target. if you include the ROW data you pick another classifier w/the smallest average error; classifier C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>satisficing and optimizing</h6>\n",
    "<img src=\"mlstrat4c.png\">\n",
    "you can define a metric conbining 2 metrics accuracy and running time. He shows an equation where he linearly weights\n",
    "the running time $cost=accuracy - 0.3runningtime$. This seems artificial. Or you can choose classifier that maximizes \n",
    "accuracy but the runtime has to be less than 100ms. Accuracy is optimizing metric and running time is satificing metric. A\n",
    "satisficing metric is something that is good enough. Dont care as much as accuracy. The optimizing metric is what you\n",
    "run the classifiers to and you pick the ones which meet the satisficing metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>train/dev/test distributions</h6>\n",
    "The following examples shoes a dev error in different distribution (geographically) than the test set. Describes\n",
    "this as having a different target for the programmers to train to. Have to mix up the training data to incluce from \n",
    "all regions. \n",
    "<img src=\"mlstrat5a.png\">\n",
    "<img src=\"mlstrat5b.png\">\n",
    "<img src=\"mlstrat5c.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>size of dev and test sets</h6>\n",
    "<img src=\"mlstrat6a.png\">\n",
    "<img src=\"mlstrat6b.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>when to change dev/test sets and metrics</h6>\n",
    "Example of a cat classifier A that has 3% test error but lets through porn and B which is 5% but doesnt let through porn. \n",
    "What to do with these misclassifications? Even though it is 3% still worse than B which has a higher error rate. \n",
    "\n",
    "<img src=\"mlstrat7a.png\">\n",
    "The evaluation metric we use is the indicator function I which is a 0 when ypred==yi or predicted is same as test \n",
    "set. When predicted is not equal to actual then we have an error or 1. Problem w this metric hs th porn error are \n",
    "treated teh same as other errors. \n",
    "<img src=\"mlstrat7b.png\">\n",
    "Add a weighting factor. If not getting the correct rank order preference time to think of new evaluation metric. \n",
    "Given 2 classifiers teh goal of the metric is to get which one is better. \n",
    "<img src=\"mlstrat7c.png\">\n",
    "This is one example of orthogonizilation. If you have to hcnage the metric to weight the loss function differentlh. We\n",
    "saw this w/the porn cat classifier. \n",
    "<img src=\"mlstrat7d.png\">\n",
    "When you train the cat classifier you find A is better when training on nicely focused images of cats. But B performs\n",
    "better in real life? What happened? users are uploading unfocused images and this is example of dev/test fallign down/failing\n",
    "What users care about are different set of images. If do well on metric and dev/test sets and if it doesnt correspond to\n",
    "real life than change metric. You can augment teh data set also. Chagne metric or change dev/test set. \n",
    "<img src=\"mlstrat7e.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>why human level performance</h6>\n",
    "once you go past human performance you cant get above bayes optimal error. This is the perfect error. Line speech \n",
    "recognition or image recognition impossible for some blurry examples to be classified. Bayes error is the very best\n",
    "theoretical performance. Progress is quite fast up to human performance then progress slows down. Reason for this, first\n",
    "human error not that lower then bayes error. As long as perormance worse than human level there are tools for you to use\n",
    "to make perf better. \n",
    "<img src=\"mlstrat8a.png\">\n",
    "IF ML is worse than humans, you can get more labeled data from humans to increase performance. Nanual error analysis, \n",
    "as long as humans are better you can ask humans and use this to improve performance. You can also do better analysis\n",
    "of bias/variance. \n",
    "<img src=\"mlstrat8b.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>avoidable bias</h6>\n",
    "In the example on teh left because the TE is much greather than the human error and because this difference\n",
    "is bigger than the difference between the Test error and dev error it is easy to know to focus on reducinn training error. \n",
    "On the left example the training error is close to human error and the difference between test error and dev error is \n",
    "greater than the difference between human error and training error so focus on reducing the test dev set error. \n",
    "<img src=\"mlstrat9a.png\">\n",
    "definition fo avoidable bias as the difference between human error and dev error and variacne as teh difference between\n",
    "dev and test error. \n",
    "<img src=\"mlstrat9b.png\">\n",
    "avoidable bias is .5 and variance..2%\n",
    "<img src=\"mlstrat9c.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>understanding human level performance</h6>\n",
    "Sometimes you dont want to do too well this is overfitting. Knowing level of human performance helps here. \n",
    "Say human perf is 1% error and TE is 8 and dev is 10 there is huge gap between TE and human you can see \n",
    "TE not as good as human. So get bigger network more data let gradient descent run longer. Focus on reducing bias. \n",
    "Another example say 7.5 human error for cat recognition. So not much TE difference betwen TE and human. Maybe reduce the difference\n",
    "variance, the difference between TE and DE. Try regularization to bring DE closer to TE. In the earlier discussion \n",
    "of bias/variance we are assuming bayes error is 0!!!!!. Human level error si proxy for Bayes optimal error. For computer vision\n",
    "tasks this is a reasonable proxy. Teh surpursing thing is with the same DE and TE on these 2 scenarios each is different\n",
    "between first wheere TE is high but on right BE is closer to TE so not that much headroom for reducing TE. In B there\n",
    "is much room for improvement for the 2%. Getting more TE or regularization. \n",
    "<img src=\"mlstrat10a.png\">\n",
    "Difference between BE and TE is avoidable bias. Keep on improving Training performance till you get to bayes error. THe \n",
    "difference between TE and DEV error is variance. THe avoidable bias acknowleges there is a minimum error you cant get\n",
    "below. There a .5% avoidable bias and a 2% variance. \n",
    "<img src=\"mlstrat10b.png\">\n",
    "u need to know what human error is first before this analysis or optimal bayes error. \n",
    "<img src=\"mlstrat10c.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>surpassing human level performance</h6>\n",
    "Lets say a typical human is 3% doc is 1% and team of experienced docs is .5%. What is the one you pick for bayes\n",
    "errror? Is typical or smallest? Smallest!!! Think of Human error is a proxy for bayes error. SO the best the bayes\n",
    "error is the experienced doctos which is the lowest system error. We dont know if we can get better than this. Butwe know\n",
    "we have a floor. USe 0.5% for bayes error. \n",
    "<img src=\"mlstrat11a.png\">\n",
    "\n",
    "<img src=\"mlstrat11b.png\">\n",
    "<img src=\"mlstrat11c.png\">\n",
    "Bayes Error is 0.5 or 1% or .7. TE is 5% and DE is 6%. Avoidable bias is BE-TE and variance is TE-DE. Measure of avoidable\n",
    "bias is 4 to 4.5%. Var is 1%. Doesnt really matter which definition of HE you use cause you only get .5% difference. \n",
    "The bias is still bigger than variance so focus on bias reduction, bigger network. Lets consider another example if\n",
    "TE is 1% and DE is 5% then avoidable bias is 0 to .5 and var is 5% so focus on variance reduction. Where it matters \n",
    "if TE is 0.7 and DE is 0.8 then it matters which human error you use. if 0.5 then ab is 0,2%. and variance is 0.1%. If you\n",
    "use 0.7 for human then you get bias of 0 and you missed you should have done better on the training set. CLEARER verify wTA\n",
    "we miss bc there is more head room if we pick another human. This problem only arose when already we are doing very well\n",
    "on the training set, we did not hand lable correctly bc we could have gotten better. On hte left it was clear what the goals\n",
    "were but on right after we have a good system not clear where to spend time if we pick wrong human experts. \n",
    "<img src=\"mlstrat11d.png\">\n",
    "Summary , human level error is proxy for bayes error and it tells you the avoidable bias and variance amounts. INstead \n",
    "of comarig 0 percdent for bias like we did earler. VERIFY W?TA bias is TE how much bigger than 0 as bias. This is ok approzimation\n",
    "if BE is near 0. For problems like speech recognition BE is not 0. \n",
    "<img src=\"mlstrat11e.png\">\n",
    "Avoidable bias is 1.9variance is 0.1. To reduce bias get more Training data or better optimization. YOu can reduce regularization\n",
    "to make the variance bigger but that has nothing to do w bias. These are indepenent knobs. \n",
    "<img src=\"mlstrat11f.png\">\n",
    "<img src=\"mlstrat11g.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>improving model performacne</h6>\n",
    "Can you surpass human level performance? \n",
    "The case on the left you know the absolute biasn is 0.1 and the variance is 0.2. Clear. On the right the training\n",
    "error is less than the team of humans error. You dont know if this is overfit or if the bayes error is less than the team\n",
    "of humans. \n",
    "<img src=\"mlstrat12a.png\">\n",
    "there are areas where ML is better than ppl....\n",
    "mostly structured data and areas where teams have access to more data than what humans can look at which makes it \n",
    "easier for the computer to look at than humans can examine. \n",
    "\n",
    "\n",
    "<img src=\"mlstrat12b.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Imprving your model performance</h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>week2 carrying out error analysis</h6>\n",
    "IF you have a dev error of 10% and you count the number of errors in the dev set by sampling and you see\n",
    "5 out of 100 are incorrectly labeled by human error then it isnt worth it becase you only increase training error\n",
    "by 5% from 10% to 9.5. \n",
    "<img src=\"error1.png\">\n",
    "on the other hand here is a counterexample; if you see 50/100 mislabeled then you can lower dev error by 5% which is \n",
    "a 50% reduction. This may be worth it. \n",
    "<img src=\"error2.png\">\n",
    "<img src=\"error3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>week2 cleaning up incorretly labeled data</h6>\n",
    "Wow, turns out DL is not sensitive to random errors. If however errors are systematic like all white dogs are being\n",
    "labeled as cats then you have a problem. \n",
    "<img src=\"clean1.png\">\n",
    "<img src=\"clean2.png\">\n",
    "IF the system has 90% accuracy then we have 10% dev set error. Then if we have 6% incorrect labels then .6 are erorrs \n",
    "from mislabels. 9.4 you can focus on fixing. Mislabels not worth it. Another example suppose you made more progress\n",
    "and you are at 2%. STill 0.6 still incorrect labels. But you are closer to where mislabels make a difference. The main purpose\n",
    "fo the dev set is to pick between 2 classifiers. 2.1 vs 1.9 is tough because you dont trust the dev set anymore if\n",
    ".6 are mislabeled. This means you have to fix. \n",
    "<img src=\"clean3.png\">\n",
    "Here are some guidelines for fixing. apply same process to dev and test set. Examine algo to see right and wrong. \n",
    "if you look at wrong only you get biased images. Might have gotten something right by lucky. May not need to fix training\n",
    "set. There are some processes you can add. Super important for dev and test set to be same. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>week2 build your first system quickly</h6>\n",
    "build first system quickly and iterate. lots of directions you can go when starting. Just build a baseline first. \n",
    "<img src=\"quick1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>week2 training and testing on different distributions</h6>\n",
    "if you have 10k images fro mobile app split 5k into training set in option 2 ans 2500 to dev and 2500 to test note the dev and test \n",
    "sets ONLY contain ommbile images and no web. option 1 is to mix web + mobile together, shuffle and split. Option 1. I \n",
    "thought option 1 was the way to go!! Wrong!!\n",
    "\n",
    "<img src=\"dev1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>week2 bias and variance w/mismatched data distributions</h6>\n",
    "Given a dataset split the training dataset into a training and training-dev dataset. The dev and test are still from\n",
    "the same distribution and are different than tehe training set. The new step is to create a traiing-dev dataset from\n",
    "the existing training data. First case: if the training error is 1% and the training-dev error is 9% then we have a variance\n",
    "    problem and if the training-dev is 1.5% and dev is 10% we have a data mismatch problem. Variance is 1$ but the data mismatch\n",
    "    is large inteh second example. \n",
    "    <p></p>\n",
    "    If human error is 0 and training error is 10% we have an avoidable bias problem \n",
    "<img src=\"traindev1.png\">\n",
    "<li>avoidable bias =  human level - training set</li>\n",
    "<li>variance = training set error - training dev error</li>\n",
    "<li>data mismatch = training dev - dev error</li>\n",
    "<li>ovrfitting to dev(variance) = dev error - test error</li>\n",
    "One example dev set is better and test eror is better. What happened? speech recognition teh training data was harder\n",
    "than test and dev set. More general formulation\n",
    "<img src=\"traindev2.png\">\n",
    "\n",
    "<img src=\"traindev3.png\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>week2 addressing data mismatch</h6>\n",
    "use rearview mirror example for ASR> \n",
    "Look at dev set error w/spreadsheet and you have noisy samples which dont classify well. This is a big number say\n",
    "50% of the dev error. Also you see noise on speech recognition task as having effect on error. How to simpuate noisy\n",
    "car data? Use data augmentation\n",
    "<img src=\"trainmis1.png\">\n",
    "Dont have a training set wiht noise. so make one. add car noise + audio words/sentence. \n",
    "theres a risk of ovefitting to 1h or car noise. How do you know this? You will see this in test error. \n",
    "\n",
    "<img src=\"trainmis2.png\">\n",
    "Example of bounding box.. want to simulate images of cars w/computer graphcis. Looks good to human eye. Synthesis can \n",
    "only make small section compared to what human eye can recognize. Grab images from video game. If game has 20 unique cars\n",
    "then the car looks fine. But the world has more than 20 unique designs of cars. If you only have20 then nn will overfit\n",
    "to the 20 cars. Even though it looks realistic then you are covering tiny subset. \n",
    "If you ahve mismatch problem do an error analysis then see if you can get more training data taht looks lmore like dev\n",
    "set. Augmentation works esp in speech but if you use it be cautious you can be overfitting. \n",
    "<img src=\"trainmis3.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>week2 transfer learning</h6>\n",
    "Delete the last layer and add randomy linitialized weights for last layer. Train all teh usual parameters all way for \n",
    "original task. Then for TF after it is trained swap in a new set of data and replace last layer. INit last layer to ranoom\n",
    "and retrain on new dataset. If you have small dataset retrain the last layer. IF lots of ddata ratrain. pretraining is\n",
    "first stage on different dataset like cat images then retrain step is fine tuning wo swapping layers. Strat from other weights\n",
    "vs random initialization. \n",
    "Speech recognition example: want to build wake word system. nihao baidu. Take out teh last layer of NN and create a new \n",
    "    outptu node. YOu can create not a single outptu but several new layers to rpedict wake word. When does TF make sense? \n",
    "    when you have a lot of data say 1m exampels for image task. Then you want to transfer to radiology w/100 examples. transferring\n",
    "    from case w/lot of data to problem w/little data. ONe place where you cant do TF is if opposite is true. \n",
    "<img src=\"trans1.png\">\n",
    "when does TF make sense? when you are trying to ransfer from taskA to B. Makes sense wehn A and B have same X. X is images \n",
    "or audio input. Has to be same input/output. \n",
    "<img src=\"trans2.png\">\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h6>week2 multitask learning</h6>\n",
    "is declan multitask learning? NEed all the tasks to have same amoutn of data because you are sharing weights. train a big network. alternative to multitask is a smaller NN for each task. You are replacing a bunch of small NNs. \n",
    "Mutli hurts only wen MT is not big enough. Should rarely hurt perf. \n",
    "<img src=\"multi1.png\">\n",
    "<img src=\"multi2.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 what is end to end dl?</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>week2 whether to use end to end</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mlstratq1.png\">\n",
    "<img src=\"mlstratq2.png\">\n",
    "<img src=\"mlstratq3.png\">\n",
    "<img src=\"mlstratq4.png\">\n",
    "<img src=\"mlstratq5.png\">\n",
    "<img src=\"mlstratq6.png\">\n",
    "<img src=\"mlstratq7.png\">\n",
    "<img src=\"mlstratq8.png\">\n",
    "<img src=\"mlstratq9.png\">\n",
    "<img src=\"mlstratq10.png\">\n",
    "<img src=\"mlstratq11.png\">\n",
    "<img src=\"mlstratq12.png\">\n",
    "<img src=\"mlstratq13.png\">\n",
    "<img src=\"mlstratq13a.png\">\n",
    "<img src=\"mlstratq14.png\">\n",
    "<img src=\"mlstratq15.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to do is to iterate quickly. From videos. \n",
    "<img src=\"mlstrat2_1.png\">\n",
    "softmax good for multiclass classification. Is this true for multitask as well? \n",
    "<img src=\"mlstrat2_2.png\">\n",
    "yes count the dev set errors using a spreadsheet and see if you ahve enough exaamples to constitute a bigger\n",
    "percentage in reducing errors. Look at both errors and good images. A little tricky if he means teh randomly chosen\n",
    "images mean good cases and error cases then the answer is different. Underspecified. \n",
    "<img src=\"mlstrat2_3.png\">\n",
    "The question mark doesnt matter if it is a random error in the training set. For example if you havea  white dog\n",
    "that gets classified as a cat. We can make the question mark an unk class like we do for word tokens not in the dictionary.\n",
    "If this a systematic error on the other hand like if all white dogs are labeled incorrectly then we are screwed. Underspecified\n",
    "if the question mark is random or systematic. Assume random. \n",
    "<img src=\"mlstrat2_4.png\">\n",
    "This is a repeat question from the previous section. Do not mix them all and shuffle unless you have something at \n",
    "same scale. this is 10x difference so in a different scale put some in test/dev which 10k or 40k should be enough then\n",
    "put the rest into training. Assume 10k is enough from previous lectures. You will know if the dev/test error is not good. \n",
    "MAKE SURE YOU WORK OUT THE EXAMPLES/CAES for this and verify. \n",
    "<img src=\"mlstrat2_5.png\">\n",
    "<img src=\"mlstrat2_6.png\">\n",
    "<img src=\"mlstrat2_7.png\">\n",
    "<img src=\"mlstrat2_8.png\">\n",
    "<img src=\"mlstrat2_9.png\">\n",
    "<img src=\"mlstrat2_10.png\">\n",
    "<img src=\"mlstrat2_11.png\">\n",
    "<img src=\"mlstrat2_12.png\">\n",
    "<img src=\"mlstrat2_13.png\">\n",
    "<img src=\"mlstrat2_14.png\">\n",
    "<img src=\"mlstrat2_15.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
